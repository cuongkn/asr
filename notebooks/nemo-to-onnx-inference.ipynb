{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-12-16T12:26:35.550192Z","iopub.status.busy":"2023-12-16T12:26:35.549932Z","iopub.status.idle":"2023-12-16T12:29:23.944158Z","shell.execute_reply":"2023-12-16T12:29:23.943122Z","shell.execute_reply.started":"2023-12-16T12:26:35.550167Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["!pip install nemo_toolkit[all]\n","!pip install onnxruntime onnxruntime-gpu # for gpu, use onnxruntime-gpu\n","!pip install boto3 botocore"]},{"cell_type":"markdown","metadata":{},"source":["**COVERT MODEL TO ONNX**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-12-16T12:29:56.705582Z","iopub.status.busy":"2023-12-16T12:29:56.705253Z","iopub.status.idle":"2023-12-16T12:30:46.490270Z","shell.execute_reply":"2023-12-16T12:30:46.489402Z","shell.execute_reply.started":"2023-12-16T12:29:56.705552Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["import nemo.collections.asr as nemo_asr\n","\n","# quartznet = nemo_asr.models.EncDecCTCModel.restore_from('/kaggle/input/vivos-manifest/quartznet_vivos_100.nemo')\n","# quartznet.export('quartznet_vivos_100.onnx')\n","\n","# citrinet = nemo_asr.models.ASRModel.restore_from('/kaggle/input/libritts-manifest/citrinet_libri_bpe_freeze.nemo')\n","# citrinet.export('citrinet_libri_bpe_freeze.onnx')\n","\n","conformer = nemo_asr.models.EncDecCTCModelBPE.restore_from('/kaggle/input/libritts-manifest/conformer_libri_bpe_unfreeze.nemo')\n","conformer.export('conformer_libri_bpe_unfreeze.onnx')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T12:30:46.491647Z","iopub.status.busy":"2023-12-16T12:30:46.491363Z","iopub.status.idle":"2023-12-16T12:30:47.130316Z","shell.execute_reply":"2023-12-16T12:30:47.129372Z","shell.execute_reply.started":"2023-12-16T12:30:46.491623Z"},"trusted":true},"outputs":[],"source":["import yaml\n","from omegaconf import DictConfig, OmegaConf\n","\n","# model_Onnx = nemo_asr.models.ASRModel.restore_from(restore_path=\"/kaggle/input/vivos-manifest/quartznet_vivos_100.nemo\", return_config = True)\n","model_Onnx = nemo_asr.models.EncDecCTCModelBPE.restore_from('/kaggle/input/libritts-manifest/conformer_libri_bpe_unfreeze.nemo', return_config = True)\n","\n","textfile = open(\"conformer_libri_bpe_unfreeze.yaml\", \"w\")\n","textfile.write(str(OmegaConf.to_yaml(model_Onnx)))\n","textfile.close()"]},{"cell_type":"markdown","metadata":{},"source":["**EXAMPLE OF USING .NEMO MODEL INSTEAD OF .ONNX**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T12:30:47.131823Z","iopub.status.busy":"2023-12-16T12:30:47.131548Z","iopub.status.idle":"2023-12-16T12:30:47.138515Z","shell.execute_reply":"2023-12-16T12:30:47.137504Z","shell.execute_reply.started":"2023-12-16T12:30:47.131799Z"},"trusted":true},"outputs":[],"source":["from jiwer import wer\n","import json\n","import pandas as pd\n","\n","\n","def convert_manifest_to_df(manifest_path):\n","    audio_filepath = list()\n","    duration = list()\n","    text = list()\n","    with open(manifest_path, encoding=\"utf8\") as f:\n","        for line in f:\n","            metadata = json.loads(line)\n","            audio_filepath.append(metadata['audio_filepath'])\n","            duration.append(metadata['duration'])\n","            text.append(metadata['text'])\n","\n","    return pd.DataFrame({'audio_filepath': audio_filepath,\n","                          'duration': duration,\n","                          'text': text})"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-12-16T12:30:47.140007Z","iopub.status.busy":"2023-12-16T12:30:47.139673Z","iopub.status.idle":"2023-12-16T12:30:47.166144Z","shell.execute_reply":"2023-12-16T12:30:47.165420Z","shell.execute_reply.started":"2023-12-16T12:30:47.139976Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["import torch\n","device = 'cuda' if torch.cuda.is_available else 'cpu'\n","conformer.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T12:30:47.167436Z","iopub.status.busy":"2023-12-16T12:30:47.167175Z","iopub.status.idle":"2023-12-16T12:32:02.902262Z","shell.execute_reply":"2023-12-16T12:32:02.901288Z","shell.execute_reply.started":"2023-12-16T12:30:47.167414Z"},"trusted":true},"outputs":[],"source":["df = convert_manifest_to_df('/kaggle/input/libritts-manifest/dev-other-manifest-kaggle.json')\n","audio_path = df['audio_filepath'].tolist()\n","reference = df['text'].tolist()\n","\n","hypothesis = conformer.transcribe(paths2audio_files=audio_path)\n","print(wer(reference, hypothesis))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T12:32:02.903980Z","iopub.status.busy":"2023-12-16T12:32:02.903354Z","iopub.status.idle":"2023-12-16T12:33:13.165088Z","shell.execute_reply":"2023-12-16T12:33:13.164153Z","shell.execute_reply.started":"2023-12-16T12:32:02.903952Z"},"trusted":true},"outputs":[],"source":["df = convert_manifest_to_df('/kaggle/input/libritts-manifest/dev-clean-manifest-kaggle.json')\n","audio_path = df['audio_filepath'].tolist()\n","reference = df['text'].tolist()\n","\n","hypothesis = conformer.transcribe(paths2audio_files=audio_path)\n","print(wer(reference, hypothesis))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T12:45:25.543122Z","iopub.status.busy":"2023-12-16T12:45:25.542744Z","iopub.status.idle":"2023-12-16T12:45:25.616138Z","shell.execute_reply":"2023-12-16T12:45:25.615276Z","shell.execute_reply.started":"2023-12-16T12:45:25.543088Z"},"trusted":true},"outputs":[],"source":["hypothesis = conformer.transcribe(paths2audio_files=[\"/kaggle/input/libritts/test-clean/test-clean/1089/134686/1089-134686-0000.wav\"])\n","hypothesis"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from pydub import AudioSegment\n","import os\n","import wave\n","\n","\n","def concatenate_wav_files(input_folder, output_file):\n","    concatenated_audio = AudioSegment.silent(duration=0)\n","\n","    for foldername, _, filenames in os.walk(input_folder):\n","        for filename in filenames:\n","            if filename.endswith(\".wav\"):\n","                current_audio = AudioSegment.from_wav(os.path.join(foldername, filename))\n","                concatenated_audio += current_audio\n","    concatenated_audio.export(output_file, format=\"wav\")\n","    \n","def extract_first_minute(input_file, time):\n","    minute = time/60000\n","    output_file = input_file + f\"_{minute}.wav\"\n","    audio = AudioSegment.from_file(input_file, format=\"wav\")\n","\n","    first_minute = audio[:time]\n","\n","    # Export the first minute to a new file\n","    first_minute.export(output_file, format=\"wav\")\n","\n","def get_wav_duration(file_path):\n","    with wave.open(file_path, 'rb') as wav_file:\n","        # Get the number of frames and the frame rate\n","        num_frames = wav_file.getnframes()\n","        frame_rate = wav_file.getframerate()\n","\n","        # Calculate the duration in seconds\n","        duration = num_frames / float(frame_rate)\n","        \n","        return duration"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["concatenate_wav_files('/kaggle/input/vivos-vietnamese-speech-corpus-for-asr/vivos/test/waves', 'output.wav')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["extract_first_minute('/kaggle/working/output.wav', 600000)\n","extract_first_minute('/kaggle/working/output.wav', 300000)\n","extract_first_minute('/kaggle/working/output.wav', 900000)\n","extract_first_minute('/kaggle/working/output.wav', 1200000)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["file_path = '/kaggle/working/output.wav_20.0.wav'\n","duration = get_wav_duration(file_path)\n","print(f'The duration of the WAV file is {duration:.2f} seconds.')"]},{"cell_type":"markdown","metadata":{},"source":["**HELPERS**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T12:40:56.464966Z","iopub.status.busy":"2023-12-16T12:40:56.463964Z","iopub.status.idle":"2023-12-16T12:40:56.535957Z","shell.execute_reply":"2023-12-16T12:40:56.535217Z","shell.execute_reply.started":"2023-12-16T12:40:56.464927Z"},"trusted":true},"outputs":[],"source":["config_path = \"/kaggle/working/conformer_libri_bpe_unfreeze.yaml\"\n","with open(config_path) as f:\n","    params_conformer = yaml.safe_load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T12:41:13.070033Z","iopub.status.busy":"2023-12-16T12:41:13.069670Z","iopub.status.idle":"2023-12-16T12:41:13.385445Z","shell.execute_reply":"2023-12-16T12:41:13.384502Z","shell.execute_reply.started":"2023-12-16T12:41:13.070006Z"},"trusted":true},"outputs":[],"source":["from nemo.collections.asr.models.ctc_models import EncDecCTCModel\n","from nemo.collections.asr.metrics.wer import CTCDecoding\n","\n","import yaml\n","from omegaconf import DictConfig, OmegaConf\n","\n","\n","config_path = \"/kaggle/working/stt_en_conformer_ctc_small.yaml\"\n","with open(config_path) as f:\n","    params = yaml.safe_load(f)\n","    \n","\n","preprocessor_cfg = DictConfig(params).preprocessor\n","preprocessor = EncDecCTCModel.from_config_dict(preprocessor_cfg)\n","\n","labels = params['decoder']['vocabulary']\n","\n","decoding_cfg = DictConfig(params_conformer).decoding\n","decoding = CTCDecoding(decoding_cfg, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T12:41:18.409116Z","iopub.status.busy":"2023-12-16T12:41:18.408408Z","iopub.status.idle":"2023-12-16T12:41:18.418906Z","shell.execute_reply":"2023-12-16T12:41:18.418061Z","shell.execute_reply.started":"2023-12-16T12:41:18.409072Z"},"trusted":true},"outputs":[],"source":["def to_numpy(tensor):\n","    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n","\n","def setup_transcribe_dataloader(cfg, vocabulary):\n","    config = {\n","        'manifest_filepath': os.path.join(cfg['temp_dir'], 'manifest.json'),\n","        'sample_rate': 16000,\n","        'labels': vocabulary,\n","        'batch_size': min(cfg['batch_size'], len(cfg['paths2audio_files'])),\n","        'trim_silence': True,\n","        'shuffle': False,\n","    }\n","    dataset = AudioToCharDataset(\n","        manifest_filepath=config['manifest_filepath'],\n","        labels=config['labels'],\n","        sample_rate=config['sample_rate'],\n","        int_values=config.get('int_values', False),\n","        augmentor=None,\n","        max_duration=config.get('max_duration', None),\n","        min_duration=config.get('min_duration', None),\n","        max_utts=config.get('max_utts', 0),\n","        blank_index=config.get('blank_index', -1),\n","        unk_index=config.get('unk_index', -1),\n","        normalize=config.get('normalize_transcripts', False),\n","        trim=config.get('trim_silence', True),\n","        parser=config.get('parser', 'en'),\n","    )\n","    return torch.utils.data.DataLoader(\n","        dataset=dataset,\n","        batch_size=config['batch_size'],\n","        collate_fn=dataset.collate_fn,\n","        drop_last=config.get('drop_last', False),\n","        shuffle=False,\n","        num_workers=config.get('num_workers', 0),\n","        pin_memory=config.get('pin_memory', False),\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["**.WAV INFERENCE FILE**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T12:41:20.225023Z","iopub.status.busy":"2023-12-16T12:41:20.224301Z","iopub.status.idle":"2023-12-16T12:41:20.229119Z","shell.execute_reply":"2023-12-16T12:41:20.228148Z","shell.execute_reply.started":"2023-12-16T12:41:20.224992Z"},"trusted":true},"outputs":[],"source":["audio_file = \"/kaggle/input/libritts/test-clean/test-clean/1089/134686/1089-134686-0000.wav\""]},{"cell_type":"markdown","metadata":{},"source":["**ONNX INFERENCE**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-16T12:43:23.258511Z","iopub.status.busy":"2023-12-16T12:43:23.257855Z","iopub.status.idle":"2023-12-16T12:43:24.581107Z","shell.execute_reply":"2023-12-16T12:43:24.580072Z","shell.execute_reply.started":"2023-12-16T12:43:23.258479Z"},"trusted":true},"outputs":[],"source":["import onnxruntime\n","import tempfile\n","import os\n","import json\n","import numpy as np\n","import torch\n","from nemo.collections.asr.metrics.wer import WER\n","from nemo.collections.asr.data.audio_to_text import AudioToCharDataset\n","\n","\n","# ort_session = onnxruntime.InferenceSession('/kaggle/working/quartznet_vivos_100.onnx', providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'])\n","ort_session = onnxruntime.InferenceSession('/kaggle/working/stt_en_conformer_ctc_small.onnx')\n","\n","with tempfile.TemporaryDirectory() as tmpdir:\n","    with open(os.path.join(tmpdir, 'manifest.json'), 'w') as fp:\n","        for audio_file in [audio_file]:\n","            entry = {'audio_filepath': audio_file, 'duration': 100000, 'text': 'nothing'}\n","            fp.write(json.dumps(entry) + '\\n')\n","\n","    config = {'paths2audio_files': [audio_file], 'batch_size': 4, 'temp_dir': tmpdir}\n","    temporary_datalayer = setup_transcribe_dataloader(config, labels)\n","    for test_batch in temporary_datalayer:\n","        processed_signal, processed_signal_len = preprocessor(\n","            input_signal=test_batch[0], length=test_batch[1]\n","        )\n","        ort_inputs = {\n","            ort_session.get_inputs()[0].name: to_numpy(processed_signal), \n","            ort_session.get_inputs()[1].name: to_numpy(processed_signal_len)\n","        }\n","        ologits = ort_session.run(None, ort_inputs)\n","        alogits = np.asarray(ologits)\n","        logits = torch.from_numpy(alogits[0])\n","        greedy_predictions = logits.argmax(dim=-1, keepdim=False)\n","        wer = WER(decoding=decoding, use_cer=False)\n","        hypotheses, _ = wer.decoding.ctc_decoder_predictions_tensor(greedy_predictions)\n","        hypotheses = [hypothesis.replace(\"▁\", \" \")[1:] for hypothesis in hypotheses]\n","        print(hypotheses)\n","        break"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2703147,"sourceId":4653379,"sourceType":"datasetVersion"},{"datasetId":4166447,"sourceId":7202473,"sourceType":"datasetVersion"},{"datasetId":4013624,"sourceId":7206588,"sourceType":"datasetVersion"},{"datasetId":4166496,"sourceId":7215571,"sourceType":"datasetVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
